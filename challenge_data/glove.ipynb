{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc='Preprocessing')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download some NLP models for processing\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load GloVe model with Gensim's API\n",
    "embeddings_model = api.load(\"glove-twitter-200\")  # 200-dimensional GloVe embeddings\n",
    "\n",
    "# Function to compute the average word vector for a tweet\n",
    "def get_avg_embedding(tweet, model, vector_size=200):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # Lowercasing\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+|www\\.\\S+', '', tweet)\n",
    "    \n",
    "    # Remove mentions (@username)\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    \n",
    "    # Remove hashtags (keep the text after the #)\n",
    "    tweet = re.sub(r'#(\\w+)', r'\\1', tweet)\n",
    "    \n",
    "    # Remove special characters, punctuation, and numbers\n",
    "    tweet = re.sub(r'[^a-z\\s]', '', tweet)\n",
    "    \n",
    "    # Tokenization\n",
    "    words = tweet.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Handle emojis (optional: convert to text or remove)\n",
    "    tweet = emoji.demojize(' '.join(words))  # Converts emojis to text, e.g., \":smile:\"\n",
    "    \n",
    "    # Final cleanup: remove redundant spaces\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all training files and concatenate them into one dataframe\n",
    "li = []\n",
    "for filename in os.listdir(\"train_tweets\"):\n",
    "    df = pd.read_csv(\"train_tweets/\" + filename)\n",
    "    li.append(df)\n",
    "df = pd.concat(li, ignore_index=True)\n",
    "\n",
    "# Entrainer sur un petit échantillon\n",
    "# df = df.sample(n=1000000, random_state=42)\n",
    "\n",
    "# Apply preprocessing to each tweet\n",
    "df['Tweet'] = df['Tweet'].progress_apply(preprocess_tweet)\n",
    "\n",
    "# Add a feature for sentiment using TextBlob\n",
    "df['sentiment'] = df['Tweet'].progress_apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data_preprocess.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subdivision and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_preprocess.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subdivisions(df, num_subdivisions=30):\n",
    "    subdivided_data = []\n",
    "    \n",
    "    # Iterate through each period\n",
    "    for period_id, group in df.groupby(['MatchID', 'PeriodID']):\n",
    "        tweets_per_period = len(group)\n",
    "        subdivision_size = tweets_per_period // num_subdivisions\n",
    "        \n",
    "        # Process each subdivision\n",
    "        for i in range(num_subdivisions):\n",
    "            # Get the subset of tweets for this subdivision\n",
    "            start_idx = i * subdivision_size\n",
    "            end_idx = (i + 1) * subdivision_size if (i + 1) * subdivision_size <= tweets_per_period else tweets_per_period\n",
    "            \n",
    "            # Ensure we have data in the range (handle edge cases for small periods)\n",
    "            if start_idx >= end_idx:\n",
    "                continue\n",
    "            \n",
    "            subdivision_tweets = group.iloc[start_idx:end_idx]\n",
    "            \n",
    "            # Get the embeddings for the tweets in this subdivision\n",
    "            try:\n",
    "                embeddings = np.vstack([get_avg_embedding(tweet, embeddings_model) for tweet in subdivision_tweets['Tweet']])\n",
    "                \n",
    "                # Calculate the average embedding for the subdivision\n",
    "                avg_embedding = np.mean(embeddings, axis=0)\n",
    "                \n",
    "                # Append the data for this subdivision (one sample per subdivision)\n",
    "                subdivided_data.append({\n",
    "                    'MatchID': group['MatchID'].iloc[0],\n",
    "                    'PeriodID': group['PeriodID'].iloc[0],\n",
    "                    'sentiment': subdivision_tweets['sentiment'].mean(),  # Average sentiment\n",
    "                    'tweet_vectors': avg_embedding, \n",
    "                    'EventType': group['EventType'].iloc[0]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        \n",
    "    # Convert the list of subdivided data into a DataFrame\n",
    "    return pd.DataFrame(subdivided_data)\n",
    "\n",
    "# Create subdivisions and process the data\n",
    "df_subdivided = create_subdivisions(df)\n",
    "\n",
    "# Check the size of the new dataset\n",
    "print(f\"Number of rows in the subdivided dataset: {len(df_subdivided)}\")\n",
    "\n",
    "# Extract tweet vectors (embeddings)\n",
    "tweet_vectors = np.vstack(df_subdivided['tweet_vectors'].values)\n",
    "\n",
    "# Extract additional features: PeriodID and sentiment\n",
    "period_id_feature = df_subdivided['PeriodID'].values.reshape(-1, 1)\n",
    "sentiment_feature = df_subdivided['sentiment'].values.reshape(-1, 1)\n",
    "\n",
    "# Combine all features into a single X array\n",
    "X = np.hstack([tweet_vectors, period_id_feature, sentiment_feature])\n",
    "\n",
    "# Output the shape of the final feature array\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "\n",
    "y = df_subdivided['EventType'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Define the PyTorch MLP model with LeakyReLU\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(202, 200) \n",
    "        self.fc2 = nn.Linear(200, 100)\n",
    "        self.fc3 = nn.Linear(100, 50)\n",
    "        self.fc4 = nn.Linear(50, 25)\n",
    "        self.fc5 = nn.Linear(25, 2)  # Output size based on the number of classes\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)  # LeakyReLU activation\n",
    "        self.softmax = nn.Softmax(dim=1)  # For multi-class classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        x = self.leaky_relu(self.fc3(x))\n",
    "        x = self.leaky_relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Add a scheduler to adjust the learning rate\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Training loop with scheduler\n",
    "epochs = 40\n",
    "batch_size = 32\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i in range(0, len(X_train_tensor), batch_size):\n",
    "        # Batch processing\n",
    "        X_batch = X_train_tensor[i:i+batch_size]\n",
    "        y_batch = y_train_tensor[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        current_lr = optimizer.param_groups[0]['lr']  # Get current learning rate\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_prob = model(X_test_tensor)\n",
    "    y_pred = torch.argmax(y_pred_prob, axis=1).numpy()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## KAGGLE ###########################################\n",
    "\n",
    "# Lire les fichiers CSV dans eval_tweets et concaténer dans un DataFrame\n",
    "eval_data = []\n",
    "for filename in os.listdir(\"eval_tweets\"):\n",
    "    filepath = os.path.join(\"eval_tweets\", filename)\n",
    "    if os.path.isfile(filepath):  # Vérifier que c'est bien un fichier\n",
    "        df = pd.read_csv(filepath)\n",
    "        eval_data.append(df)\n",
    "\n",
    "eval_df = pd.concat(eval_data, ignore_index=True)\n",
    "\n",
    "# Appliquer le prétraitement et ajouter des caractéristiques\n",
    "eval_df['Tweet'] = eval_df['Tweet'].progress_apply(preprocess_tweet)\n",
    "eval_df['sentiment'] = eval_df['Tweet'].progress_apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_csv('eval_preprocess.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Create subdivisions for the test set\n",
    "df_subdivided_test = create_subdivisions_test(eval_df)\n",
    "\n",
    "# Step 2: Prepare the feature tensor for prediction\n",
    "tweet_vectors_test = np.vstack(df_subdivided_test['tweet_vectors'].values)\n",
    "period_id_feature_test = df_subdivided_test['PeriodID'].values.reshape(-1, 1)\n",
    "sentiment_feature_test = df_subdivided_test['sentiment'].values.reshape(-1, 1)\n",
    "\n",
    "# Combine features\n",
    "X_test = np.hstack([tweet_vectors_test, period_id_feature_test, sentiment_feature_test])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Step 3: Predict labels for each subdivision using the neural network\n",
    "# Assuming `model` is your trained PyTorch neural network\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "\n",
    "# Add predictions to the DataFrame\n",
    "df_subdivided_test['PredictedEventType'] = predicted_labels.numpy()\n",
    "\n",
    "# Step 4: Aggregate predictions to get one label per minute\n",
    "def aggregate_predictions_nn(df):\n",
    "    final_predictions = []\n",
    "    for (match_id, period_id), group in df.groupby(['MatchID', 'PeriodID']):\n",
    "        # Use majority voting to determine the label for the minute\n",
    "        predicted_labels = group['PredictedEventType']\n",
    "        most_common_label = Counter(predicted_labels).most_common(1)[0][0]\n",
    "        \n",
    "        # Append the aggregated result\n",
    "        final_predictions.append({\n",
    "            'ID': group['ID'].iloc[0],  # Keep the ID of the minute\n",
    "            'PredictedEventType': most_common_label\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(final_predictions)\n",
    "\n",
    "# Apply aggregation\n",
    "aggregated_predictions_nn = aggregate_predictions_nn(df_subdivided_test)\n",
    "\n",
    "# Step 5: Output results\n",
    "print(f\"Number of aggregated predictions: {len(aggregated_predictions_nn)}\")\n",
    "print(aggregated_predictions_nn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_predictions_nn.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concaténation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def get_avg_embedding(tweet, model, vector_size=200):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to subdivide data into 20 intervals and concatenate embeddings\n",
    "def create_subdivisions_with_concatenation(df, num_subdivisions=20, embeddings_model=None):\n",
    "    subdivided_data = []\n",
    "    \n",
    "    # Convertir la colonne Tweet en chaînes pour éviter les erreurs\n",
    "    df['Tweet'] = df['Tweet'].astype(str)  # Convertir toutes les valeurs en chaînes\n",
    "\n",
    "    # Group by MatchID and PeriodID\n",
    "    for (match_id, period_id), group in df.groupby(['MatchID', 'PeriodID']):\n",
    "        tweets_per_period = len(group)\n",
    "        subdivision_size = tweets_per_period // num_subdivisions\n",
    "        \n",
    "        # Placeholder for concatenated embeddings\n",
    "        concatenated_embeddings = []\n",
    "        \n",
    "        for i in range(num_subdivisions):\n",
    "            # Get the subset of tweets for this subdivision\n",
    "            start_idx = i * subdivision_size\n",
    "            end_idx = (i + 1) * subdivision_size if (i + 1) * subdivision_size <= tweets_per_period else tweets_per_period\n",
    "            \n",
    "            if start_idx >= end_idx:  # Handle edge cases\n",
    "                continue\n",
    "            \n",
    "            subdivision_tweets = group.iloc[start_idx:end_idx]\n",
    "            \n",
    "            # Compute average embedding for this subdivision\n",
    "            embeddings = []\n",
    "            for tweet in subdivision_tweets['Tweet']:\n",
    "                try:\n",
    "                    # Replace this with your actual embedding model logic\n",
    "                    embeddings.append(get_avg_embedding(tweet, embeddings_model))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing tweet: {tweet} | Error: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if embeddings:\n",
    "                avg_embedding = np.mean(np.vstack(embeddings), axis=0)\n",
    "                concatenated_embeddings.append(avg_embedding)\n",
    "        \n",
    "        # Flatten concatenated embeddings into a single vector\n",
    "        if concatenated_embeddings:\n",
    "            flattened_embeddings = np.concatenate(concatenated_embeddings)\n",
    "            \n",
    "            subdivided_data.append({\n",
    "                'MatchID': match_id,\n",
    "                'PeriodID': period_id,\n",
    "                'ID': group['ID'].iloc[0],  # Keep the first ID\n",
    "                'ConcatenatedEmbeddings': flattened_embeddings,\n",
    "                'EventType': group['EventType'].iloc[0]  # Assuming same EventType for the whole period\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(subdivided_data)\n",
    "\n",
    "df = pd.read_csv('data_preprocess.csv')\n",
    "\n",
    "df_subdivided = create_subdivisions_with_concatenation(df, num_subdivisions=20, embeddings_model=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subdivided.to_csv('data_subdivisee.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_random_forest_classifier_with_grid_search(df):\n",
    "    # Prepare features\n",
    "    embeddings = np.vstack(df['ConcatenatedEmbeddings'].values)\n",
    "    period_ids = df['PeriodID'].values.reshape(-1, 1)  # Reshape to match dimensions for concatenation\n",
    "    \n",
    "    # Concatenate PeriodID as an additional feature\n",
    "    X = np.hstack([embeddings, period_ids])  # X now has shape (n_samples, 4001)\n",
    "    \n",
    "    # Prepare labels\n",
    "    y = df['EventType']\n",
    "    \n",
    "    # Encode labels if necessary\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Define the Random Forest Classifier\n",
    "    clf = RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    # Define the parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 150],  # Réduire les valeurs testées\n",
    "        'max_depth': [20, None], \n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [3, 5],\n",
    "        'max_features': ['sqrt']\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Set up the GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=clf,\n",
    "        param_grid=param_grid,\n",
    "        scoring='accuracy',\n",
    "        cv=5,  # 5-fold cross-validation\n",
    "        verbose=2,  # Increase output verbosity for debugging\n",
    "        n_jobs=-1   # Use all available processors\n",
    "    )\n",
    "    \n",
    "    # Perform the grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best estimator\n",
    "    best_clf = grid_search.best_estimator_\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Evaluate the classifier with the best parameters\n",
    "    y_pred = best_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Optimized Random Forest Classifier Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return best_clf, label_encoder, grid_search.best_params_\n",
    "\n",
    "# Step 2: Train the Random Forest Classifier with Grid Search\n",
    "# df_subdivided = pd.read_csv('data_subdivisee.csv')\n",
    "\n",
    "rf, label_encoder, best_params = train_random_forest_classifier_with_grid_search(df_subdivided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour subdiviser et concaténer les embeddings dans le jeu de données d'évaluation\n",
    "eval_df = pd.read_csv('eval_preprocess.csv')\n",
    "def prepare_eval_data_with_concatenation(eval_df, num_subdivisions=20, embeddings_model=None):\n",
    "    prepared_data = []\n",
    "    \n",
    "    # Convertir la colonne Tweet en chaînes pour éviter les erreurs\n",
    "    eval_df['Tweet'] = eval_df['Tweet'].astype(str)\n",
    "    \n",
    "    for (match_id, period_id), group in eval_df.groupby(['MatchID', 'PeriodID']):\n",
    "        tweets_per_period = len(group)\n",
    "        subdivision_size = tweets_per_period // num_subdivisions\n",
    "        \n",
    "        concatenated_embeddings = []\n",
    "        \n",
    "        for i in range(num_subdivisions):\n",
    "            # Get the subset of tweets for this subdivision\n",
    "            start_idx = i * subdivision_size\n",
    "            end_idx = (i + 1) * subdivision_size if (i + 1) * subdivision_size <= tweets_per_period else tweets_per_period\n",
    "            \n",
    "            if start_idx >= end_idx:\n",
    "                continue\n",
    "            \n",
    "            subdivision_tweets = group.iloc[start_idx:end_idx]\n",
    "            \n",
    "            # Compute average embedding for this subdivision\n",
    "            embeddings = []\n",
    "            for tweet in subdivision_tweets['Tweet']:\n",
    "                try:\n",
    "                    embeddings.append(get_avg_embedding(tweet, embeddings_model))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing tweet: {tweet} | Error: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if embeddings:\n",
    "                avg_embedding = np.mean(np.vstack(embeddings), axis=0)\n",
    "                concatenated_embeddings.append(avg_embedding)\n",
    "        \n",
    "        # Flatten concatenated embeddings into a single vector\n",
    "        if concatenated_embeddings:\n",
    "            flattened_embeddings = np.concatenate(concatenated_embeddings)\n",
    "            \n",
    "            prepared_data.append({\n",
    "                'ID': group['ID'].iloc[0],  # Keep the first ID\n",
    "                'ConcatenatedEmbeddings': flattened_embeddings,\n",
    "                'PeriodID': period_id\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(prepared_data)\n",
    "\n",
    "eval_prepared = prepare_eval_data_with_concatenation(eval_df, num_subdivisions=20, embeddings_model=embeddings_model)\n",
    "\n",
    "# Prepare features\n",
    "# Stack embeddings and add PeriodID as an additional feature\n",
    "embeddings = np.vstack(eval_prepared['ConcatenatedEmbeddings'].values)\n",
    "period_ids = eval_prepared['PeriodID'].values.reshape(-1, 1)  # Reshape to match dimensions for concatenation\n",
    "\n",
    "# Concatenate PeriodID as an additional feature\n",
    "X_eval = np.hstack([embeddings, period_ids])\n",
    "predicted_labels_rf = rf.predict(X_eval)\n",
    "\n",
    "# Ajouter les prédictions au DataFrame\n",
    "eval_prepared['EventType'] = predicted_labels_rf\n",
    "\n",
    "# Extraire les colonnes ID et EventType pour l'export\n",
    "result_df = eval_prepared[['ID', 'EventType']]\n",
    "\n",
    "# Exporter le résultat au format CSV\n",
    "result_df.to_csv('eval_predictions_rf.csv', index=False)\n",
    "\n",
    "print(\"Les prédictions ont été enregistrées dans 'eval_predictions.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Best Parameters: {'activation': 'relu', 'hidden_layer_sizes': (4000, 1000, 500, 200), 'solver': 'adam'}\n",
      "Optimized MLP Classifier Accuracy: 1.00\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def train_mlp_classifier_with_grid_search(df):\n",
    "    # Prepare features\n",
    "    embeddings = np.vstack(df['ConcatenatedEmbeddings'].values)\n",
    "    period_ids = df['PeriodID'].values.reshape(-1, 1)  # Reshape to match dimensions for concatenation\n",
    "    \n",
    "    # Concatenate PeriodID as an additional feature\n",
    "    X = np.hstack([embeddings, period_ids])  # X now has shape (n_samples, 4001)\n",
    "    \n",
    "    # Standardize the features (important for MLPs)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Prepare labels\n",
    "    y = df['EventType']\n",
    "    \n",
    "    # Encode labels if necessary\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Define the MLP Classifier\n",
    "    clf = MLPClassifier(random_state=42, max_iter=500)\n",
    "    \n",
    "    # Define the parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'hidden_layer_sizes': [(2048, 512, 128, 64), (4000, 1000, 500, 200)],  # Different layer configurations\n",
    "        'activation': ['relu'],                   # Activation functions\n",
    "        'solver': ['adam'],                        # Solvers\n",
    "        # 'learning_rate_init': [0.001, 0.01],              # Initial learning rates\n",
    "        # 'alpha': [0.0001, 0.001],                         # L2 regularization parameter\n",
    "    }\n",
    "    \n",
    "    # Set up the GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=clf,\n",
    "        param_grid=param_grid,\n",
    "        scoring='accuracy',\n",
    "        cv=5,  # 5-fold cross-validation\n",
    "        verbose=2,  # Increase output verbosity for debugging\n",
    "        n_jobs=-1   # Use all available processors\n",
    "    )\n",
    "    \n",
    "    # Perform the grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best estimator\n",
    "    best_clf = grid_search.best_estimator_\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Evaluate the classifier with the best parameters\n",
    "    y_pred = best_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Optimized MLP Classifier Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return best_clf, label_encoder, scaler, grid_search.best_params_\n",
    "\n",
    "# Example usage:\n",
    "# df_subdivided = pd.read_csv('data_subdivisee.csv')\n",
    "mlp, label_encoder, scaler, best_params = train_mlp_classifier_with_grid_search(df_subdivided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les prédictions ont été enregistrées dans 'eval_predictions_mlp.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Prepare features for evaluation\n",
    "embeddings = np.vstack(eval_prepared['ConcatenatedEmbeddings'].values)\n",
    "period_ids = eval_prepared['PeriodID'].values.reshape(-1, 1)  # Reshape to match dimensions for concatenation\n",
    "\n",
    "# Concatenate PeriodID as an additional feature\n",
    "X_eval = np.hstack([embeddings, period_ids])\n",
    "\n",
    "# Standardize the evaluation data using the same scaler as for training\n",
    "# X_eval = scaler.transform(X_eval)  # Ensure the data is normalized similarly to training\n",
    "\n",
    "# Predict labels using the trained MLP model\n",
    "predicted_labels_encoded = mlp.predict(X_eval)\n",
    "\n",
    "# Decode the predicted labels back to original class names\n",
    "predicted_labels_mlp = label_encoder.inverse_transform(predicted_labels_encoded)\n",
    "\n",
    "# Ajouter les prédictions au DataFrame\n",
    "eval_prepared['EventType'] = predicted_labels_mlp\n",
    "\n",
    "# Extraire les colonnes ID et EventType pour l'export\n",
    "result_df = eval_prepared[['ID', 'EventType']]\n",
    "\n",
    "# Exporter le résultat au format CSV\n",
    "result_df.to_csv('eval_predictions_mlp.csv', index=False)\n",
    "\n",
    "print(\"Les prédictions ont été enregistrées dans 'eval_predictions_mlp.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf554",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
