{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc='Preprocessing')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download some NLP models for processing\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load GloVe model with Gensim's API\n",
    "embeddings_model = api.load(\"glove-twitter-200\")  # 200-dimensional GloVe embeddings\n",
    "\n",
    "# Function to compute the average word vector for a tweet\n",
    "def get_avg_embedding(tweet, model, vector_size=200):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # Lowercasing\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+|www\\.\\S+', '', tweet)\n",
    "    \n",
    "    # Remove mentions (@username)\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    \n",
    "    # Remove hashtags (keep the text after the #)\n",
    "    tweet = re.sub(r'#(\\w+)', r'\\1', tweet)\n",
    "    \n",
    "    # Remove special characters, punctuation, and numbers\n",
    "    tweet = re.sub(r'[^a-z\\s]', '', tweet)\n",
    "    \n",
    "    # Tokenization\n",
    "    words = tweet.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Handle emojis (optional: convert to text or remove)\n",
    "    tweet = emoji.demojize(' '.join(words))  # Converts emojis to text, e.g., \":smile:\"\n",
    "    \n",
    "    # Final cleanup: remove redundant spaces\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all training files and concatenate them into one dataframe\n",
    "li = []\n",
    "for filename in os.listdir(\"train_tweets\"):\n",
    "    df = pd.read_csv(\"train_tweets/\" + filename)\n",
    "    li.append(df)\n",
    "df = pd.concat(li, ignore_index=True)\n",
    "\n",
    "# Entrainer sur un petit échantillon\n",
    "# df = df.sample(n=1000000, random_state=42)\n",
    "\n",
    "# Apply preprocessing to each tweet\n",
    "df['Tweet'] = df['Tweet'].progress_apply(preprocess_tweet)\n",
    "\n",
    "# Add a feature for sentiment using TextBlob\n",
    "df['sentiment'] = df['Tweet'].progress_apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data_preprocess.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concaténation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def get_avg_embedding(tweet, model, vector_size=200):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to subdivide data into 20 intervals and concatenate embeddings\n",
    "def create_subdivisions_with_concatenation(df, num_subdivisions=20, embeddings_model=None):\n",
    "    subdivided_data = []\n",
    "    \n",
    "    # Convertir la colonne Tweet en chaînes pour éviter les erreurs\n",
    "    df['Tweet'] = df['Tweet'].astype(str)  # Convertir toutes les valeurs en chaînes\n",
    "\n",
    "    # Group by MatchID and PeriodID\n",
    "    for (match_id, period_id), group in df.groupby(['MatchID', 'PeriodID']):\n",
    "        tweets_per_period = len(group)\n",
    "        subdivision_size = tweets_per_period // num_subdivisions\n",
    "        \n",
    "        # Placeholder for concatenated embeddings\n",
    "        concatenated_embeddings = []\n",
    "        \n",
    "        for i in range(num_subdivisions):\n",
    "            # Get the subset of tweets for this subdivision\n",
    "            start_idx = i * subdivision_size\n",
    "            end_idx = (i + 1) * subdivision_size if (i + 1) * subdivision_size <= tweets_per_period else tweets_per_period\n",
    "            \n",
    "            if start_idx >= end_idx:  # Handle edge cases\n",
    "                continue\n",
    "            \n",
    "            subdivision_tweets = group.iloc[start_idx:end_idx]\n",
    "            \n",
    "            # Compute average embedding for this subdivision\n",
    "            embeddings = []\n",
    "            for tweet in subdivision_tweets['Tweet']:\n",
    "                try:\n",
    "                    # Replace this with your actual embedding model logic\n",
    "                    embeddings.append(get_avg_embedding(tweet, embeddings_model))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing tweet: {tweet} | Error: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if embeddings:\n",
    "                avg_embedding = np.mean(np.vstack(embeddings), axis=0)\n",
    "                concatenated_embeddings.append(avg_embedding)\n",
    "        \n",
    "        # Flatten concatenated embeddings into a single vector\n",
    "        if concatenated_embeddings:\n",
    "            flattened_embeddings = np.concatenate(concatenated_embeddings)\n",
    "            \n",
    "            subdivided_data.append({\n",
    "                'MatchID': match_id,\n",
    "                'PeriodID': period_id,\n",
    "                'ID': group['ID'].iloc[0],  # Keep the first ID\n",
    "                'ConcatenatedEmbeddings': flattened_embeddings,\n",
    "                'EventType': group['EventType'].iloc[0]  # Assuming same EventType for the whole period\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(subdivided_data)\n",
    "\n",
    "df = pd.read_csv('data_preprocess.csv')\n",
    "\n",
    "df_subdivided = create_subdivisions_with_concatenation(df, num_subdivisions=20, embeddings_model=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subdivided.to_csv('data_subdivisee.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_random_forest_classifier_with_grid_search(df):\n",
    "    # Prepare features\n",
    "    embeddings = np.vstack(df['ConcatenatedEmbeddings'].values)\n",
    "    period_ids = df['PeriodID'].values.reshape(-1, 1)  # Reshape to match dimensions for concatenation\n",
    "    \n",
    "    # Concatenate PeriodID as an additional feature\n",
    "    X = np.hstack([embeddings, period_ids])  # X now has shape (n_samples, 4001)\n",
    "    \n",
    "    # Prepare labels\n",
    "    y = df['EventType']\n",
    "    \n",
    "    # Encode labels if necessary\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Define the Random Forest Classifier\n",
    "    clf = RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    # Define the parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 150],  # Réduire les valeurs testées\n",
    "        'max_depth': [20, None], \n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [3, 5],\n",
    "        'max_features': ['sqrt']\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Set up the GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=clf,\n",
    "        param_grid=param_grid,\n",
    "        scoring='accuracy',\n",
    "        cv=5,  # 5-fold cross-validation\n",
    "        verbose=2,  # Increase output verbosity for debugging\n",
    "        n_jobs=-1   # Use all available processors\n",
    "    )\n",
    "    \n",
    "    # Perform the grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best estimator\n",
    "    best_clf = grid_search.best_estimator_\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Evaluate the classifier with the best parameters\n",
    "    y_pred = best_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Optimized Random Forest Classifier Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return best_clf, label_encoder, grid_search.best_params_\n",
    "\n",
    "# Step 2: Train the Random Forest Classifier with Grid Search\n",
    "# df_subdivided = pd.read_csv('data_subdivisee.csv')\n",
    "\n",
    "rf, label_encoder, best_params = train_random_forest_classifier_with_grid_search(df_subdivided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour subdiviser et concaténer les embeddings dans le jeu de données d'évaluation\n",
    "eval_df = pd.read_csv('eval_preprocess.csv')\n",
    "def prepare_eval_data_with_concatenation(eval_df, num_subdivisions=20, embeddings_model=None):\n",
    "    prepared_data = []\n",
    "    \n",
    "    # Convertir la colonne Tweet en chaînes pour éviter les erreurs\n",
    "    eval_df['Tweet'] = eval_df['Tweet'].astype(str)\n",
    "    \n",
    "    for (match_id, period_id), group in eval_df.groupby(['MatchID', 'PeriodID']):\n",
    "        tweets_per_period = len(group)\n",
    "        subdivision_size = tweets_per_period // num_subdivisions\n",
    "        \n",
    "        concatenated_embeddings = []\n",
    "        \n",
    "        for i in range(num_subdivisions):\n",
    "            # Get the subset of tweets for this subdivision\n",
    "            start_idx = i * subdivision_size\n",
    "            end_idx = (i + 1) * subdivision_size if (i + 1) * subdivision_size <= tweets_per_period else tweets_per_period\n",
    "            \n",
    "            if start_idx >= end_idx:\n",
    "                continue\n",
    "            \n",
    "            subdivision_tweets = group.iloc[start_idx:end_idx]\n",
    "            \n",
    "            # Compute average embedding for this subdivision\n",
    "            embeddings = []\n",
    "            for tweet in subdivision_tweets['Tweet']:\n",
    "                try:\n",
    "                    embeddings.append(get_avg_embedding(tweet, embeddings_model))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing tweet: {tweet} | Error: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if embeddings:\n",
    "                avg_embedding = np.mean(np.vstack(embeddings), axis=0)\n",
    "                concatenated_embeddings.append(avg_embedding)\n",
    "        \n",
    "        # Flatten concatenated embeddings into a single vector\n",
    "        if concatenated_embeddings:\n",
    "            flattened_embeddings = np.concatenate(concatenated_embeddings)\n",
    "            \n",
    "            prepared_data.append({\n",
    "                'ID': group['ID'].iloc[0],  # Keep the first ID\n",
    "                'ConcatenatedEmbeddings': flattened_embeddings,\n",
    "                'PeriodID': period_id\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(prepared_data)\n",
    "\n",
    "eval_prepared = prepare_eval_data_with_concatenation(eval_df, num_subdivisions=20, embeddings_model=embeddings_model)\n",
    "\n",
    "# Prepare features\n",
    "# Stack embeddings and add PeriodID as an additional feature\n",
    "embeddings = np.vstack(eval_prepared['ConcatenatedEmbeddings'].values)\n",
    "period_ids = eval_prepared['PeriodID'].values.reshape(-1, 1)  # Reshape to match dimensions for concatenation\n",
    "\n",
    "# Concatenate PeriodID as an additional feature\n",
    "X_eval = np.hstack([embeddings, period_ids])\n",
    "predicted_labels_rf = rf.predict(X_eval)\n",
    "\n",
    "# Ajouter les prédictions au DataFrame\n",
    "eval_prepared['EventType'] = predicted_labels_rf\n",
    "\n",
    "# Extraire les colonnes ID et EventType pour l'export\n",
    "result_df = eval_prepared[['ID', 'EventType']]\n",
    "\n",
    "# Exporter le résultat au format CSV\n",
    "result_df.to_csv('eval_predictions_rf.csv', index=False)\n",
    "\n",
    "print(\"Les prédictions ont été enregistrées dans 'eval_predictions.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Best Parameters: {'activation': 'relu', 'hidden_layer_sizes': (4000, 1000, 500, 200), 'solver': 'adam'}\n",
      "Optimized MLP Classifier Accuracy: 1.00\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def train_mlp_classifier_with_grid_search(df):\n",
    "    # Prepare features\n",
    "    embeddings = np.vstack(df['ConcatenatedEmbeddings'].values)\n",
    "    period_ids = df['PeriodID'].values.reshape(-1, 1)  # Reshape to match dimensions for concatenation\n",
    "    \n",
    "    # Concatenate PeriodID as an additional feature\n",
    "    X = np.hstack([embeddings, period_ids])  # X now has shape (n_samples, 4001)\n",
    "    \n",
    "    # Standardize the features (important for MLPs)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Prepare labels\n",
    "    y = df['EventType']\n",
    "    \n",
    "    # Encode labels if necessary\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Define the MLP Classifier\n",
    "    clf = MLPClassifier(random_state=42, max_iter=500)\n",
    "    \n",
    "    # Define the parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'hidden_layer_sizes': [(2048, 512, 128, 64), (4000, 1000, 500, 200)],  # Different layer configurations\n",
    "        'activation': ['relu'],                   # Activation functions\n",
    "        'solver': ['adam'],                        # Solvers\n",
    "        # 'learning_rate_init': [0.001, 0.01],              # Initial learning rates\n",
    "        # 'alpha': [0.0001, 0.001],                         # L2 regularization parameter\n",
    "    }\n",
    "    \n",
    "    # Set up the GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=clf,\n",
    "        param_grid=param_grid,\n",
    "        scoring='accuracy',\n",
    "        cv=5,  # 5-fold cross-validation\n",
    "        verbose=2,  # Increase output verbosity for debugging\n",
    "        n_jobs=-1   # Use all available processors\n",
    "    )\n",
    "    \n",
    "    # Perform the grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best estimator\n",
    "    best_clf = grid_search.best_estimator_\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Evaluate the classifier with the best parameters\n",
    "    y_pred = best_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Optimized MLP Classifier Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return best_clf, label_encoder, scaler, grid_search.best_params_\n",
    "\n",
    "# Example usage:\n",
    "# df_subdivided = pd.read_csv('data_subdivisee.csv')\n",
    "mlp, label_encoder, scaler, best_params = train_mlp_classifier_with_grid_search(df_subdivided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les prédictions ont été enregistrées dans 'eval_predictions_mlp.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Prepare features for evaluation\n",
    "embeddings = np.vstack(eval_prepared['ConcatenatedEmbeddings'].values)\n",
    "period_ids = eval_prepared['PeriodID'].values.reshape(-1, 1)  # Reshape to match dimensions for concatenation\n",
    "\n",
    "# Concatenate PeriodID as an additional feature\n",
    "X_eval = np.hstack([embeddings, period_ids])\n",
    "\n",
    "# Standardize the evaluation data using the same scaler as for training\n",
    "# X_eval = scaler.transform(X_eval)  # Ensure the data is normalized similarly to training\n",
    "\n",
    "# Predict labels using the trained MLP model\n",
    "predicted_labels_encoded = mlp.predict(X_eval)\n",
    "\n",
    "# Decode the predicted labels back to original class names\n",
    "predicted_labels_mlp = label_encoder.inverse_transform(predicted_labels_encoded)\n",
    "\n",
    "# Ajouter les prédictions au DataFrame\n",
    "eval_prepared['EventType'] = predicted_labels_mlp\n",
    "\n",
    "# Extraire les colonnes ID et EventType pour l'export\n",
    "result_df = eval_prepared[['ID', 'EventType']]\n",
    "\n",
    "# Exporter le résultat au format CSV\n",
    "result_df.to_csv('eval_predictions_mlp.csv', index=False)\n",
    "\n",
    "print(\"Les prédictions ont été enregistrées dans 'eval_predictions_mlp.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf554",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
